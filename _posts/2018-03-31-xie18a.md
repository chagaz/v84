---
title: Towards Memory-Friendly Deterministic Incremental Gradient Method
abstract: 'Incremental Gradient (IG) methods are classical strategies in solving finite
  sum minimization problems. Deterministic IG methods are particularly favorable in
  handling massive scale problem due to its memory-friendly data access pattern. In
  this paper, we propose a new deterministic variant of the IG method SVRG that blends
  a periodically updated full gradient with a component function gradient selected
  in a cyclic order. Our method uses only $O(1)$ extra gradient storage without compromising
  the linear convergence. Empirical results demonstrate that the proposed method is
  advantageous over existing incremental gradient algorithms, especially on problems
  that does not fit into physical memory. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: xie18a
month: 0
tex_title: Towards Memory-Friendly Deterministic Incremental Gradient Method
firstpage: 1147
lastpage: 1156
page: 1147-1156
order: 1147
cycles: false
author:
- given: Jiahao
  family: Xie
- given: Hui
  family: Qian
- given: Zebang
  family: Shen
- given: Chao
  family: Zhang
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/xie18a/xie18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/xie18a/xie18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
