---
title: A Generic Approach for Escaping Saddle points
abstract: A central challenge to using first-order methods for optimizing nonconvex
  problems is the presence of saddle points. First-order methods often get stuck at
  saddle points, greatly deteriorating their performance. Typically, to escape from
  saddles one has to use second-order methods. However, most works on second-order
  methods rely extensively on expensive Hessian-based computations, making them impractical
  in large-scale settings. To tackle this challenge, we introduce a generic framework
  that minimizes Hessian-based computations while at the same time provably converging
  to second-order critical points. Our framework carefully alternates between a first-order
  and a second-order subroutine, using the latter only close to saddle points, and
  yields convergence results competitive to the state-of-the-art. Empirical results
  suggest that our strategy also enjoys a good practical performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: reddi18a
month: 0
tex_title: A Generic Approach for Escaping Saddle points
firstpage: 1233
lastpage: 1242
page: 1233-1242
order: 1233
cycles: false
author:
- given: Sashank
  family: Reddi
- given: Manzil
  family: Zaheer
- given: Suvrit
  family: Sra
- given: Barnabas
  family: Poczos
- given: Francis
  family: Bach
- given: Ruslan
  family: Salakhutdinov
- given: Alex
  family: Smola
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/reddi18a/reddi18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/reddi18a/reddi18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
