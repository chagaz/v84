---
title: 'Conditional Gradient Method for Stochastic Submodular Maximization: Closing
  the Gap'
abstract: In this paper, we study the problem of constrained and stochastic continuous
  submodular maximization. Even though the objective function is not concave (nor
  convex) and is defined in terms of an expectation, we develop a variant of the conditional
  gradient method, called Stochastic Continuous Greedy, which achieves a tight approximation
  guarantee. More precisely, for a monotone and continuous DR-submodular function
  and subject to a general convex body constraint, we prove that Stochastic Continuous
  Greedy achieves a $[(1-1/e)\text{OPT} -\eps]$ guarantee (in expectation) with $\mathcal{O}{(1/\eps^3)}$
  stochastic gradient computations. This guarantee matches the known hardness results
  and closes the gap between deterministic and stochastic continuous submodular maximization.
  By using stochastic continuous optimization as an interface, we also provide the
  first $(1-1/e)$ tight approximation guarantee for maximizing a monotone but stochastic
  submodular set function subject to a general matroid constraint.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mokhtari18a
month: 0
tex_title: 'Conditional Gradient Method for Stochastic Submodular Maximization: Closing
  the Gap'
firstpage: 1886
lastpage: 1895
page: 1886-1895
order: 1886
cycles: false
author:
- given: Aryan
  family: Mokhtari
- given: Hamed
  family: Hassani
- given: Amin
  family: Karbasi
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/mokhtari18a/mokhtari18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/mokhtari18a/mokhtari18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
