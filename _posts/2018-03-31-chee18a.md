---
title: Convergence diagnostics for stochastic gradient descent with constant learning
  rate
abstract: Many iterative procedures in stochastic optimization exhibit a transient
  phase followed by a stationary phase. During the transient phase the procedure converges
  towards a region of interest, and during the stationary phase the procedure oscillates
  in that region, commonly around a single point. In this paper, we develop a statistical
  diagnostic test to detect such phase transition in the context of stochastic gradient
  descent with constant learning rate. We present theory and experiments suggesting
  that the region where the proposed diagnostic is activated coincides with the convergence
  region. For a class of loss functions, we derive a closed-form solution describing
  such region. Finally, we suggest an application to speed up convergence of stochastic
  gradient descent by halving the learning rate each time stationarity is detected.
  This leads to a new variant of stochastic gradient descent, which in many settings
  is comparable to state-of-art.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chee18a
month: 0
tex_title: Convergence diagnostics for stochastic gradient descent with constant learning
  rate
firstpage: 1476
lastpage: 1485
page: 1476-1485
order: 1476
cycles: false
author:
- given: Jerry
  family: Chee
- given: Panos
  family: Toulis
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/chee18a/chee18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/chee18a/chee18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
