---
title: Catalyst for Gradient-based Nonconvex Optimization
abstract: We introduce a generic scheme to solve nonconvex optimization problems using
  gradient-based algorithms originally designed for minimizing convex functions. Even
  though these methods may originally require convexity to operate, the proposed approach
  allows one to use them without assuming any knowledge about the convexity of the
  objective. In general, the scheme is guaranteed to produce a stationary point with
  a worst-case efficiency typical of first-order methods, and when the objective turns
  out to be convex, it automatically accelerates in the sense of Nesterov and achieves
  near-optimal convergence rate in function values. We conclude the paper by showing
  promising experimental results obtained by applying our approach to incremental
  algorithms such as SVRG and SAGA for sparse matrix factorization and for learning
  neural networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: paquette18a
month: 0
tex_title: Catalyst for Gradient-based Nonconvex Optimization
firstpage: 613
lastpage: 622
page: 613-622
order: 613
cycles: false
author:
- given: Courtney
  family: Paquette
- given: Hongzhou
  family: Lin
- given: Dmitriy
  family: Drusvyatskiy
- given: Julien
  family: Mairal
- given: Zaid
  family: Harchaoui
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/paquette18a/paquette18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
