---
title: Approximate Bayesian Computation with Kullback-Leibler Divergence as Data Discrepancy
abstract: Complex simulator-based models usually have intractable likelihood functions,
  rendering the likelihood-based inference methods inapplicable. Approximate Bayesian
  Computation (ABC) emerges as an alternative framework of likelihood-free inference
  methods. It identifies a quasi-posterior distribution by finding values of parameter
  that simulate the synthetic data resembling the observed data. A major ingredient
  of ABC is the discrepancy measure between the observed and the simulated data, which
  conventionally involves a fundamental difficulty of constructing effective summary
  statistics. To bypass this difficulty, we adopt a Kullback-Leibler divergence estimator
  to assess the data discrepancy. Our method enjoys the asymptotic consistency and
  linearithmic time complexity as the data size increases. In experiments on five
  benchmark models, this method achieves a comparable or higher quasi-posterior quality,
  compared to the existing methods using other discrepancy measures.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jiang18a
month: 0
tex_title: Approximate Bayesian Computation with Kullback-Leibler Divergence as Data
  Discrepancy
firstpage: 1711
lastpage: 1721
page: 1711-1721
order: 1711
cycles: false
author:
- given: Bai
  family: Jiang
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/jiang18a/jiang18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/jiang18a/jiang18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
