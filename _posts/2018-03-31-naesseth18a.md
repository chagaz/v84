---
title: Variational Sequential Monte Carlo
abstract: Many recent advances in large scale probabilistic inference rely on variational
  methods. The success of variational approaches depends on (i) formulating a flexible
  parametric family of distributions, and (ii) optimizing the parameters to find the
  member of this family that most closely approximates the exact posterior. In this
  paper we present a new approximating family of distributions, the variational sequential
  Monte Carlo (VSMC) family, and show how to optimize it in variational inference.
  VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing
  practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC
  family is a variational family that can approximate the posterior arbitrarily well,
  while still allowing for efficient optimization of its parameters. We demonstrate
  its utility on state space models, stochastic volatility models for financial data,
  and deep Markov models of brain neural circuits.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: naesseth18a
month: 0
tex_title: Variational Sequential Monte Carlo
firstpage: 968
lastpage: 977
page: 968-977
order: 968
cycles: false
author:
- given: Christian
  family: Naesseth
- given: Scott
  family: Linderman
- given: Rajesh
  family: Ranganath
- given: David
  family: Blei
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/naesseth18a/naesseth18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/naesseth18a/naesseth18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
