---
title: Semi-Supervised Prediction-Constrained Topic Models
abstract: 'Supervisory signals can help topic models discover low-dimensional data
  representations which are useful for a specific prediction task. We propose a framework
  for training supervised latent Dirichlet allocation that balances two goals: faithful
  generative explanations of high-dimensional data and accurate prediction of associated
  class labels. Existing approaches fail to balance these goals by not properly handling
  a fundamental asymmetry: the intended application is always predicting labels from
  data, not data from labels. Our new prediction-constrained objective for training
  generative models coherently integrates supervisory signals even when only a small
  fraction of training examples are labeled. We demonstrate improved prediction quality
  compared to previous supervised topic models, achieving results competitive with
  high-dimensional logistic regression on text analysis and electronic health records
  tasks while simultaneously learning interpretable topics. '
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hughes18a
month: 0
tex_title: Semi-Supervised Prediction-Constrained Topic Models
firstpage: 1067
lastpage: 1076
page: 1067-1076
order: 1067
cycles: false
author:
- given: Michael
  family: Hughes
- given: Gabriel
  family: Hope
- given: Leah
  family: Weiner
- given: Thomas
  family: McCoy
- given: Roy
  family: Perlis
- given: Erik
  family: Sudderth
- given: Finale
  family: Doshi-Velez
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/hughes18a/hughes18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/hughes18a/hughes18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
