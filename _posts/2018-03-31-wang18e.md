---
title: Stochastic Zeroth-order Optimization in High Dimensions
abstract: 'We consider the problem of optimizing a high-dimensional convex function
  using stochastic zeroth-order queries. Under sparsity assumptions on the gradients
  or function values, we present two algorithms: a successive component/feature selection
  algorithm and a noisy mirror descent algorithm using Lasso gradient estimates, and
  show that both algorithms have convergence rates that depend only logarithmically
  on the ambient dimension of the problem. Empirical results confirm our theoretical
  findings and show that the algorithms we design outperform classical zeroth-order
  optimization methods in the high-dimensional setting.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wang18e
month: 0
tex_title: Stochastic Zeroth-order Optimization in High Dimensions
firstpage: 1356
lastpage: 1365
page: 1356-1365
order: 1356
cycles: false
author:
- given: Yining
  family: Wang
- given: Simon
  family: Du
- given: Sivaraman
  family: Balakrishnan
- given: Aarti
  family: Singh
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/wang18e/wang18e.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/wang18e/wang18e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
