---
title: 'Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in Distributed
  SGD'
abstract: Distributed Stochastic Gradient Descent (SGD) when run in a synchronous
  manner, suffers from delays in waiting for the slowest learners (stragglers). Asynchronous
  methods can alleviate stragglers, but cause gradient staleness that can adversely
  affect convergence. In this work we present the first theoretical characterization
  of the speed-up offered by asynchronous methods by analyzing the trade-off between
  the error in the trained model and the actual training runtime (wallclock time).
  The novelty in our work is that our runtime analysis considers random straggler
  delays, which helps us design and compare distributed SGD algorithms that strike
  a balance between stragglers and staleness. We also present a new convergence analysis
  of asynchronous SGD variants without bounded or exponential delay assumptions, and
  a novel learning rate schedule to compensate for gradient staleness.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: dutta18a
month: 0
tex_title: 'Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs in
  Distributed SGD'
firstpage: 803
lastpage: 812
page: 803-812
order: 803
cycles: false
author:
- given: Sanghamitra
  family: Dutta
- given: Gauri
  family: Joshi
- given: Soumyadip
  family: Ghosh
- given: Parijat
  family: Dube
- given: Priya
  family: Nagpurkar
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/dutta18a/dutta18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/dutta18a/dutta18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
