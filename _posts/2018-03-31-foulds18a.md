---
title: Mixed Membership Word Embeddings for Computational Social Science
abstract: Word embeddings improve the performance of NLP systems by revealing the
  hidden structural relationships between words. Despite their success in many applications,
  word embeddings have seen very little use in computational social science NLP tasks,
  presumably due to their reliance on big data, and to a lack of interpretability.
  I propose a probabilistic model-based word embedding method which can recover interpretable
  embeddings, without big data. The key insight is to leverage mixed membership modeling,
  in which global representations are shared, but individual entities (i.e. dictionary
  words) are free to use these representations to uniquely differing degrees. I show
  how to train the model using a combination of state-of-the-art training techniques
  for word embeddings and topic models. The experimental results show an improvement
  in predictive language modeling of up to 63% in MRR over the skip-gram, and demonstrate
  that the representations are beneficial for supervised learning. I illustrate the
  interpretability of the models with computational social science case studies on
  State of the Union addresses and NIPS articles.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: foulds18a
month: 0
tex_title: Mixed Membership Word Embeddings for Computational Social Science
firstpage: 86
lastpage: 95
page: 86-95
order: 86
cycles: false
author:
- given: James
  family: Foulds
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/foulds18a/foulds18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/foulds18a/foulds18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
