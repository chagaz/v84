---
title: Factorized Recurrent Neural Architectures for Longer Range Dependence
abstract: The ability to capture Long Range Dependence (LRD) in a stochastic process
  is of prime importance in the context of predictive models. A sequential model with
  a longer-term memory is better able contextualize recent observations. In this article,
  we apply the theory of LRD stochastic processes to modern recurrent architectures,
  such as LSTMs and GRUs, and prove they do not provide LRD under assumptions sufficient
  for gradients to vanish. Motivated by an information-theoretic analysis, we provide
  a modified recurrent neural architecture that mitigates the issue of faulty memory
  through redundancy while keeping the compute time constant. Experimental results
  on a synthetic copy task, the Youtube-8m video classification task and a recommender
  system show that we enable better memorization and longer-term memory.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: belletti18a
month: 0
tex_title: Factorized Recurrent Neural Architectures for Longer Range Dependence
firstpage: 1522
lastpage: 1530
page: 1522-1530
order: 1522
cycles: false
author:
- given: Francois
  family: Belletti
- given: Alex
  family: Beutel
- given: Sagar
  family: Jain
- given: Ed
  family: Chi
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/belletti18a/belletti18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
