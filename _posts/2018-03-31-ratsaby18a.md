---
title: On how complexity affects the stability of a predictor
abstract: Given a finite random sample from a Markov chain environment, we select
  a predictor that minimizes a criterion function and refer to it as being calibrated
  to its environment. If its prediction error is not bounded by its criterion value,
  we say that the criterion fails. We define the predictor’s complexity to be the
  amount of uncertainty in detecting that the criterion fails given that it fails.
  We define a predictor’s stability to be the discrepancy between the average number
  of prediction errors that it makes on two random samples. We show that complexity
  is inversely proportional to the level of adaptivity of the calibrated predictor
  to its random environment. The calibrated predictor becomes less stable as its complexity
  increases or as its level of adaptivity decreases.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ratsaby18a
month: 0
tex_title: On how complexity affects the stability of a predictor
firstpage: 161
lastpage: 167
page: 161-167
order: 161
cycles: false
author:
- given: Joel
  family: Ratsaby
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/ratsaby18a/ratsaby18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
