---
title: VAE with a VampPrior
abstract: Many different methods to train deep generative models have been introduced
  in the past. In this paper, we propose to extend the variational auto-encoder (VAE)
  framework with a new type of prior which we call "Variational Mixture of Posteriors"
  prior, or VampPrior for short. The VampPrior consists of a mixture distribution
  (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned
  on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical
  model and show that this architecture with a coupled prior and posterior, learns
  significantly better models. The model also avoids the usual local optima issues
  related to useless latent dimensions that plague VAEs. We provide empirical studies
  on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,
  Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior
  delivers state-of-the-art results on all datasets in the unsupervised permutation
  invariant setting and the best results or comparable to SOTA methods for the approach
  with convolutional networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: tomczak18a
month: 0
tex_title: VAE with a VampPrior
firstpage: 1214
lastpage: 1223
page: 1214-1223
order: 1214
cycles: false
author:
- given: Jakub
  family: Tomczak
- given: Max
  family: Welling
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/tomczak18a/tomczak18a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v84/tomczak18a/tomczak18a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
