---
title: Learning Priors for Invariance
abstract: 'Informative priors are often difficult, if not impossible, to elicit for
  modern large-scale Bayesian models.  Yet, often, some prior knowledge is known,
  and this information is incorporated via engineering tricks or methods less principled
  than a Bayesian prior.  However, employing these tricks is difficult to reconcile
  with principled probabilistic inference.  For instance, in the case of data set
  augmentation, the posterior is conditioned on artificial data and not on what is
  actually observed.  In this paper, we address the problem of how to specify an informative
  prior when the problem of interest is known to exhibit invariance properties.  The
  proposed method is akin to posterior variational inference: we choose a parametric
  family and optimize to find the member of the family that makes the model robust
  to a given transformation.  We demonstrate the methodâ€™s utility for dropout and
  rotation transformations, showing that the use of these priors results in performance
  competitive to that of non-Bayesian methods.  Furthermore, our approach does not
  depend on the data being labeled and thus can be used in semi-supervised settings.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: nalisnick18a
month: 0
tex_title: Learning Priors for Invariance
firstpage: 366
lastpage: 375
page: 366-375
order: 366
cycles: false
author:
- given: Eric
  family: Nalisnick
- given: Padhraic
  family: Smyth
date: 2018-03-31
address: 
publisher: PMLR
container-title: Proceedings of the Twenty-First International Conference on Artificial
  Intelligence and Statistics
volume: '84'
genre: inproceedings
issued:
  date-parts:
  - 2018
  - 3
  - 31
pdf: http://proceedings.mlr.press/v84/nalisnick18a/nalisnick18a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
